{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 176, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images=tf.data.Dataset.list_files('Alzheimer_s Dataset/test/NonDemented/*.jpg',shuffle=False)\n",
    "\n",
    "def load_img(x):\n",
    "    byte=tf.io.read_file(x)\n",
    "    img=tf.io.decode_jpeg(byte,channels=3)\n",
    "    return img\n",
    "\n",
    "images=images.map(load_img)\n",
    "images.as_numpy_iterator().next().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(data): \n",
    "    image = data['image']\n",
    "    return image / 255\n",
    "\n",
    "images=tf.data.Dataset.list_files('Alzheimer_s Dataset/test/NonDemented/*.jpg',shuffle=False)\n",
    "images=images.map(load_img)\n",
    "\n",
    "images = images.map(lambda x: tf.image.resize(x, (56,56)))\n",
    "images = images.map(lambda x: x/255)\n",
    "images = images.cache()\n",
    "\n",
    "# Batch into 128 images per sample\n",
    "images = images.batch(4)\n",
    "# Reduces the likelihood of bottlenecking \n",
    "images = images.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 56, 56, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D\n",
    "\n",
    "def build_generator(): \n",
    "    model = Sequential([\n",
    "        # Takes in random values and reshapes it to 7x7x128\n",
    "        # Beginnings of a generated image\n",
    "        Dense(14*14*64, input_dim=64),\n",
    "        LeakyReLU(0.2),\n",
    "        Reshape((14,14,64)),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(256, 5, padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(256, 5, padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "\n",
    "        Conv2D(128, 4, padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "        \n",
    "        Conv2D(64, 4, padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "        \n",
    "        # Conv layer to get to one channel\n",
    "        Conv2D(3, 4, padding='same', activation='sigmoid')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 12544)             815360    \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 256)       409856    \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 56, 56, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 56, 56, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 56, 56, 256)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 56, 56, 128)       524416    \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 56, 56, 64)        131136    \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 56, 56, 3)         3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,522,499\n",
      "Trainable params: 3,522,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(): \n",
    "    model = Sequential([\n",
    "        Conv2D(32, 5, input_shape = (56,56,3)),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(64, 5),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(128, 5),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(256, 5),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 52, 52, 32)        2432      \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 52, 52, 32)        0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 52, 52, 32)        0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 48, 48, 64)        51264     \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 44, 44, 128)       204928    \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 44, 44, 128)       0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 44, 44, 128)       0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 40, 40, 256)       819456    \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 40, 40, 256)       0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 40, 40, 256)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 409600)            0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 409600)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 409601    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,487,681\n",
      "Trainable params: 1,487,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "g_opt = Adam(learning_rate=0.0001) \n",
    "d_opt = Adam(learning_rate=0.00001) \n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionGAN(Model): \n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_images = batch\n",
    "        fake_images = self.generator(tf.random.normal((64,64,1)), training=False)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake images to the discriminator model\n",
    "            yhat_real = self.discriminator(real_images, training=True) \n",
    "            yhat_fake = self.discriminator(fake_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            \n",
    "            # Create labels for real and fakes images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new images\n",
    "            gen_images = self.generator(tf.random.normal((64,64,1)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of subclassed model\n",
    "fashgan = FashionGAN(generator, discriminator)\n",
    "\n",
    "# Compile the model\n",
    "fashgan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=64):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join('images', f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 60s 374ms/step - d_loss: 0.3689 - g_loss: 1.9323\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 63s 394ms/step - d_loss: 0.2777 - g_loss: 2.4443\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 67s 418ms/step - d_loss: 0.3047 - g_loss: 0.2432\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 72s 452ms/step - d_loss: 0.2739 - g_loss: 0.4142\n",
      "Epoch 5/20\n",
      "160/160 [==============================] - 74s 462ms/step - d_loss: 0.2728 - g_loss: 0.5041\n",
      "Epoch 6/20\n",
      "160/160 [==============================] - 74s 461ms/step - d_loss: 0.2718 - g_loss: 0.5854\n",
      "Epoch 7/20\n",
      "160/160 [==============================] - 72s 449ms/step - d_loss: 0.2724 - g_loss: 0.6617\n",
      "Epoch 8/20\n",
      "160/160 [==============================] - 72s 447ms/step - d_loss: 0.2993 - g_loss: 0.6726\n",
      "Epoch 9/20\n",
      "160/160 [==============================] - 72s 450ms/step - d_loss: 0.3503 - g_loss: 0.4636\n",
      "Epoch 10/20\n",
      "160/160 [==============================] - 72s 452ms/step - d_loss: 0.3738 - g_loss: 0.4705\n",
      "Epoch 11/20\n",
      "160/160 [==============================] - 75s 466ms/step - d_loss: 0.3679 - g_loss: 0.4684\n",
      "Epoch 12/20\n",
      "160/160 [==============================] - 74s 465ms/step - d_loss: 0.3691 - g_loss: 0.5574\n",
      "Epoch 13/20\n",
      "160/160 [==============================] - 76s 472ms/step - d_loss: 0.3742 - g_loss: 0.6203\n",
      "Epoch 14/20\n",
      "160/160 [==============================] - 75s 470ms/step - d_loss: 0.3828 - g_loss: 0.6552\n",
      "Epoch 15/20\n",
      "160/160 [==============================] - 73s 457ms/step - d_loss: 0.3725 - g_loss: 0.6387\n",
      "Epoch 16/20\n",
      "160/160 [==============================] - 74s 462ms/step - d_loss: 0.3748 - g_loss: 0.6673\n",
      "Epoch 17/20\n",
      "160/160 [==============================] - 74s 460ms/step - d_loss: 0.3864 - g_loss: 0.6901\n",
      "Epoch 18/20\n",
      "160/160 [==============================] - 73s 458ms/step - d_loss: 0.3855 - g_loss: 0.7235\n",
      "Epoch 19/20\n",
      "160/160 [==============================] - 76s 476ms/step - d_loss: 0.3754 - g_loss: 0.7160\n",
      "Epoch 20/20\n",
      "160/160 [==============================] - 76s 473ms/step - d_loss: 0.3778 - g_loss: 0.7150\n"
     ]
    }
   ],
   "source": [
    "# Recommend 2000 epochs\n",
    "hist = fashgan.fit(images, epochs=20)#, callbacks=[ModelMonitor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7cd1a3a91448925d52e40248ab3fc9222a58eec32dc182ec3aef8fbd589352c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
